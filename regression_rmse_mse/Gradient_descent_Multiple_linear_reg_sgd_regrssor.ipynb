{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJ+FTh2SWEsX2eDdv3ypBc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keshav123333/Machine_learning_100_campusx/blob/main/regression_rmse_mse/Gradient_descent_Multiple_linear_reg_sgd_regrssor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "basic se kari hai isme jo Linear reg wali class vo ause cal karti hai kaunsi line wegith bias internally same result"
      ],
      "metadata": {
        "id": "IKfZWRkzNcoT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PntBcfr9MlkU"
      },
      "outputs": [],
      "source": [
        "class mullinear:\n",
        "  def __init__(self):\n",
        "    self.coef__=None\n",
        "    self.bias=None\n",
        "\n",
        "\n",
        "  def fit(self,x,y):\n",
        "    x=np.insert(x,0,1,axis=1)\n",
        "    betas=np.linalg.inv(np.dot(x.T,x)).dot(x.T).dot(y)\n",
        "    self.coef_=betas[1:]\n",
        "    self.bias_=betas[0]\n",
        "\n",
        "\n",
        "  def predict(self,xtest):\n",
        "     return np.dot(self.coef__,xtest)+self.bias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sgd regressor bhi linear reg hi bas diff itna ki sgd weight cal ke liye gradient descent kihelp leti hai"
      ],
      "metadata": {
        "id": "bzq7rf-xOs1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sgd regressor built from scratch"
      ],
      "metadata": {
        "id": "NSarUNx_M4or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "i_hzW52jOj-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y=make_regression(n_samples=100,n_informative=1,n_targets=1,noise=20)"
      ],
      "metadata": {
        "id": "PI-e2BAtNFbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# -----------------------------------------\n",
        "# STEP 1: Generate synthetic regression data\n",
        "# -----------------------------------------\n",
        "X, y = make_regression(n_samples=1000, n_features=5, noise=20, random_state=42)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize data (VERY IMPORTANT for SGD)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# -----------------------------------------\n",
        "# STEP 2: Create the SGDRegressor model\n",
        "# -----------------------------------------\n",
        "model = SGDRegressor(\n",
        "    loss='squared_error',      # Loss function: 'squared_error' = Ordinary Least Squares (default)\n",
        "                               # 'huber' = robust to outliers\n",
        "                               # 'epsilon_insensitive' = SVR-like loss\n",
        "                               # 'squared_epsilon_insensitive' = squared version of SVR loss\n",
        "\n",
        "    penalty='l2',              # Regularization term:\n",
        "                               # 'l2' = Ridge, 'l1' = Lasso, 'elasticnet' = combination of both\n",
        "\n",
        "    alpha=0.0001,              # Strength of regularization (higher value = more regularization)\n",
        "\n",
        "    max_iter=1000,             # Number of epochs (passes over data)\n",
        "\n",
        "    learning_rate='invscaling',# Learning rate schedule:\n",
        "                               # 'constant' â†’ fixed eta\n",
        "                               # 'optimal' â†’ automatically adjusted\n",
        "                               # 'invscaling' â†’ decays as learning progresses\n",
        "                               # 'adaptive' â†’ reduces only when loss stops improving\n",
        "\n",
        "    eta0=0.01,                 # Initial learning rate (used when learning_rate != 'optimal')\n",
        "\n",
        "    power_t=0.25,              # Used with 'invscaling', controls rate of decay of learning rate\n",
        "\n",
        "    early_stopping=True,       # Stop training when validation score doesnâ€™t improve\n",
        "\n",
        "    validation_fraction=0.1,   # Fraction of training data used for validation if early_stopping=True\n",
        "\n",
        "    n_iter_no_change=5,        # Number of epochs with no improvement to stop early\n",
        "\n",
        "    tol=1e-3,                  # Minimum improvement threshold for early stopping\n",
        "\n",
        "    random_state=42,           # For reproducibility\n",
        "\n",
        "    verbose=1                  # Print training progress (1 = minimal output, >1 = detailed)\n",
        ")\n",
        "\n",
        "# -----------------------------------------\n",
        "# STEP 3: Train the model\n",
        "# -----------------------------------------\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# -----------------------------------------\n",
        "# STEP 4: Predict and evaluate\n",
        "# -----------------------------------------\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nðŸ”¹ Mean Squared Error:\", mse)\n",
        "print(\"ðŸ”¹ RÂ² Score:\", r2)\n",
        "\n",
        "# -----------------------------------------\n",
        "# STEP 5: Model Info\n",
        "# -----------------------------------------\n",
        "print(\"\\nðŸ”¹ Coefficients:\", model.coef_)\n",
        "print(\"ðŸ”¹ Intercept:\", model.intercept_)\n",
        "print(\"ðŸ”¹ Number of iterations used:\", model.n_iter_)\n"
      ],
      "metadata": {
        "id": "hDDS_4_kNPAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a8ab3d-6021-4401-9c5f-723341d22d9a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 54.50, NNZs: 5, Bias: 0.037791, T: 720, Avg. loss: 596.142498\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 60.79, NNZs: 5, Bias: -0.024162, T: 1440, Avg. loss: 229.571804\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 62.55, NNZs: 5, Bias: -0.071881, T: 2160, Avg. loss: 216.272105\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 63.04, NNZs: 5, Bias: -0.157940, T: 2880, Avg. loss: 215.054586\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 63.08, NNZs: 5, Bias: -0.135093, T: 3600, Avg. loss: 214.834088\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 63.24, NNZs: 5, Bias: -0.203201, T: 4320, Avg. loss: 214.849300\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 63.25, NNZs: 5, Bias: -0.367267, T: 5040, Avg. loss: 214.680004\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 63.01, NNZs: 5, Bias: -0.266761, T: 5760, Avg. loss: 214.708797\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 8 epochs took 0.01 seconds\n",
            "\n",
            "ðŸ”¹ Mean Squared Error: 444.62443051637666\n",
            "ðŸ”¹ RÂ² Score: 0.8939564911059472\n",
            "\n",
            "ðŸ”¹ Coefficients: [25.68210699 45.77193991 15.99887378 24.10154508 19.47781871]\n",
            "ðŸ”¹ Intercept: [-0.26676148]\n",
            "ðŸ”¹ Number of iterations used: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ab maan sgd mein karna hai mini batch grad toh partia fit ye sirf ek baar chalta loop\n",
        "#sgd mein built in kuch ni hota for batch mini batch so ye use\n",
        "sgd=SGDRegressor(learning_rate=\"constant\",eta0=0.2)\n",
        "batch_size=32\n",
        "\n",
        "for i in range(100): #yaha 100 means maxir or epoch as\n",
        "  idx=np.random.choice(range(X_train.shape[0]),batch_size)\n",
        "  sgd.partial_fit(X_train[idx],y_train[idx])"
      ],
      "metadata": {
        "id": "BbHNqXrF-oMC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f3c4105",
        "outputId": "366051b5-9b14-4cf2-a9b3-010d8f5056fd"
      },
      "source": [
        "# Predict on the test set using the sgd model\n",
        "y_pred_sgd = sgd.predict(X_test)\n",
        "\n",
        "# Evaluate the sgd model\n",
        "mse_sgd = mean_squared_error(y_test, y_pred_sgd)\n",
        "r2_sgd = r2_score(y_test, y_pred_sgd)\n",
        "\n",
        "print(\"ðŸ”¹ SGD Model Mean Squared Error:\", mse_sgd)\n",
        "print(\"ðŸ”¹ SGD Model RÂ² Score:\", r2_sgd)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ SGD Model Mean Squared Error: 1532.4334343372868\n",
            "ðŸ”¹ SGD Model RÂ² Score: 0.6345126192571993\n"
          ]
        }
      ]
    }
  ]
}